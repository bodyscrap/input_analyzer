# Input Analyzer - ゲーム入力解析アプリ

ゲームの入力履歴を解析するためのGUIアプリケーションです。GStreamerで動画をデコードし、機械学習（Burn + WGPU）でフレームごとの入力を認識して、CSV形式で出力します。

## 主な機能

### 🎮 入力履歴抽出（GUIアプリ）
- **ストリーミング処理**: 1フレームずつ抽出→推論→削除でメモリ効率化
- **動画から自動抽出**: mp4動画を読み込んで入力履歴をCSVに変換
- **リアルタイム進捗表示**: フレームごとに抽出→推論→記録の進捗を表示
- **GPU推論対応**: Burn + WGPUバックエンドで高速認識
- **CSV編集機能**: 抽出したCSVを開いて手動編集・保存可能
- **キャンセル対応**: 処理中でもいつでもキャンセル可能

### 🧠 機械学習モデル
- **学習機能**: 手動分類した画像からマルチクラス分類モデルを学習
- **自動検証**: 学習中にバリデーションセットで精度を自動評価
- **モデル管理**: tar.gz形式でメタデータ付きモデルを保存・読み込み
- **クラス**: 方向9種（dir_1〜9、テンキー配列）+ ボタン（ユーザー指定の数）+ others

### ⚙️ 設定ファイル（config.json）
- 計算デバイス（GPU/CPU）の設定
- 使用モデルファイルのパス
- 入力インジケータ領域の座標・サイズ
- 前回使用した動画ファイルパス
- 次回起動時に自動的に設定を復元

## システム要件

### 必須環境
- Windows 11 (64bit)
- Rust 1.70以上
- GStreamer 1.20以上
- GPU（WGPU対応）推奨、CPUでも動作可能

### GStreamerのインストール（Windows）

1. GStreamer公式サイトからインストーラーをダウンロード: https://gstreamer.freedesktop.org/download/
2. **MSVC 64-bit**版をインストール（開発版とランタイム版の両方）
3. 環境変数を設定:
   - `GSTREAMER_1_0_ROOT_MSVC_X86_64`: GStreamerのインストールディレクトリ（例: `C:\gstreamer\1.0\msvc_x86_64\`）
   - `PATH`に`%GSTREAMER_1_0_ROOT_MSVC_X86_64%\bin`を追加

## ビルド方法

```powershell
# 全バイナリをビルド
cargo build --all-targets --features gui,ml --release

# または個別にビルド
cargo build --bin input_editor_gui --features gui,ml --release  # GUIアプリ
cargo build --bin train_model --features ml --release            # 学習ツール
```

## 使い方

### 1. 設定ファイルについて

`config.json`は設定ファイルです。存在しない場合はデフォルト設定で動作します。

**保存される設定：**
- 計算デバイス（GPU/CPU）
- モデルファイルのパス
- 解析領域設定（座標、タイルサイズ、列数）
- 前回使用した動画ファイルパス

**保存のタイミング：**
- 解析領域設定を変更した時
- 学習データ生成を実行した時
- その他、設定を変更した時

手動で`config.json`を編集することも可能です。次回起動時に設定が復元されます。

### 2. 学習データの準備

#### 2.1 解析領域の設定

GUIアプリで解析対象領域を設定：

1. GUIアプリを起動
2. メニューから「解析領域設定」を選択
3. 動画ファイル（mp4）を読み込み
4. 以下のパラメータを設定：
   - **左上座標 (X, Y)**: 入力インジケータ領域の左上座標
   - **タイルサイズ**: 正方形タイルの1辺のピクセル数（例: 48）
   - **列数**: 横方向のタイル数（例: 6または7）
5. 設定を保存（次回起動時に復元されます）

#### 2.2 タイル画像の一括抽出

GUIアプリの「学習データ生成」メニューを使用：

1. **入力動画**を選択（学習に使用するゲーム動画）
2. **出力先ルートディレクトリ**を指定（タイル画像の保存先）
3. **間引きフレーム数**を指定（例: 16 = 16フレームごとに抽出）
4. 「抽出開始」をクリック
5. 指定したディレクトリに以下の形式でタイル画像が保存されます：
   ```
   <動画ファイル名>_frame=<フレーム番号>_tile=<タイル番号>.png
   例: input_sample_06_frame=0_tile=1.png
   ```

#### 2.3 タイル画像を手動で分類

抽出されたタイル画像を手動で分類：

1. **学習ディレクトリを用意**（例: `training_data/`）
2. **直下に以下のディレクトリを作成**：
   ```
   training_data/
   ├── dir_1/          # 左下方向（固定）
   ├── dir_2/          # 下方向（固定）
   ├── dir_3/          # 右下方向（固定）
   ├── dir_4/          # 左方向（固定）
   ├── dir_6/          # 右方向（固定）
   ├── dir_7/          # 左上方向（固定）
   ├── dir_8/          # 上方向（固定）
   ├── dir_9/          # 右上方向（固定）
   ├── <任意のボタン名>/ # ユーザーが解析したいボタン（例: btn_a1, my_button など）
   └── others/         # 上記のいずれにも分類されない画像（固定）
   ```
   ※ `dir_1`〜`dir_9`（dir_5除く）と`others`は固定で作成
   ※ ボタンディレクトリ名と数は任意
3. **タイル画像を仕分け**：
   - 抽出したタイル画像から各クラス**最低1枚ずつ**抽出
   - 該当するディレクトリに画像ファイルを移動またはコピー
   - 各クラス100枚以上推奨（精度向上のため）

### 3. モデルの学習

#### 3.1 ボタンの順番を指定

GUIアプリの「モデル学習」メニューから：

1. 学習データディレクトリを選択すると、ボタンディレクトリの一覧が表示されます
   - 初回は**ディレクトリ名の昇順**で表示されます
2. ↑↓ボタンで順番を変更できます
   - ボタン名をクリックして選択
   - ↑ボタンで上に移動
   - ↓ボタンで下に移動
3. 順番を変更すると自動的に`training_data/buttons.txt`に保存されます
4. この順番がモデルのクラス順序として使用されます

※ いつでも変更可能です。次回起動時には保存された順序が使用されます

#### 3.2 学習の実行

```powershell
# デフォルト設定で学習
cargo run --bin train_model --features ml --release

# エポック数を指定
cargo run --bin train_model --features ml --release -- --epochs 50

# GPUデバイスを指定
cargo run --bin train_model --features ml --release -- --device 0
```

学習完了後、以下のファイルが生成されます：
- `models/model.tar.gz`: メタデータ付きモデル

### 4. モデルの評価と改善

学習したモデルの精度を確認し、必要に応じて改善します。

#### 4.1 タイル画像の自動分類

GUIアプリの「タイル分類と確認」メニューを使用：

1. **学習済みモデル**を選択（`models/model.tar.gz`）
2. **入力動画**を選択（検証用のゲーム動画）
3. **出力ディレクトリ**を指定（分類結果の保存先）
4. 「分類開始」をクリック
5. モデルが各タイルを自動分類し、クラスごとのディレクトリに保存します：
   ```
   output_dir/
   ├── dir_1/
   ├── dir_2/
   ...
   ├── btn_a1/
   ├── btn_a2/
   ...
   └── others/
   ```

#### 4.2 分類結果の確認と修正

1. **出力ディレクトリの画像を確認**
   - 特に誤分類されているタイルを重点的にチェック
   - 各クラスのディレクトリを開いて内容を確認

2. **誤分類の修正**
   - 誤って分類されたタイル画像を正しいディレクトリに移動
   - この作業により学習データセットが改善されます

3. **学習データへの統合**
   - 修正したタイル画像を`training_data/`の各クラスディレクトリに追加
   - 既存の学習データと統合して、より大きなデータセットを作成

#### 4.3 再学習による精度向上

1. 改善したデータセットで再度モデルを学習（セクション3を参照）
2. 再度タイル分類で精度を確認（セクション4.1）
3. 必要に応じて4.1〜4.3を繰り返す

**精度が十分になったら完成**：
- 任意の動画に対して誤分類が少なくなる
- 入力履歴抽出でも十分な精度を発揮

### 5. GUIアプリで入力履歴を抽出

```powershell
cargo run --bin input_editor_gui --features gui,ml --release
```

#### 動画から入力履歴を抽出

1. **モデル読み込み**: 「モデルを読み込む」から`models/model.tar.gz`を選択
2. **動画選択**: 「動画から抽出」から解析したいmp4ファイルを選択
3. **ストリーミング処理開始**: 自動的に抽出が始まります
   - フレーム抽出 → タイル切り出し → GPU推論 → 記録 → フレーム削除を1フレームずつ実行
   - メモリに複数フレームを保持しないため、大容量動画でも安定動作
   - プログレスバーで進捗をリアルタイム表示
4. **CSV保存**: 抽出完了後、「CSVを保存」から保存先を指定

#### CSVの手動編集

1. 「CSVを開く」から既存のCSVファイルを読み込み
2. テーブルで各行を編集（継続フレーム数、方向、ボタン）
3. 「CSVを保存」で変更を保存

### 6. ユーティリティツール

（現在、ユーティリティツールはありません）

## プロジェクト構造

```
input_analyzer/
├── src/
│   ├── lib.rs                          # ライブラリエントリポイント
│   ├── config.rs                       # 設定ファイル管理
│   ├── frame_extractor.rs              # GStreamerフレーム抽出
│   ├── input_history_extractor.rs      # 入力履歴抽出ロジック
│   ├── ml_model.rs                     # CNNモデル定義
│   ├── model_metadata.rs               # モデルメタデータ
│   ├── model_storage.rs                # モデル保存・読み込み
│   ├── inference_config.rs             # 推論設定
│   └── bin/
│       ├── input_editor_gui.rs         # メインGUIアプリ
│       └── train_model.rs              # モデル学習ツール
├── models/                             # 学習済みモデル
│   ├── model.tar.gz                    # メタデータ付きモデル
│   └── config.json                     # モデル設定
├── training_data/                      # 学習データ
│   ├── buttons.txt                     # ボタンの表示順
│   ├── dir_1/ ... dir_9/              # 方向入力画像
│   ├── <ボタン名>/                     # ボタン入力画像（ユーザー定義）
│   └── others/                         # 分類不能な画像
├── sample_data/                        # サンプル動画配置先
│   └── README.md                       # 動画要件の説明
├── output/                             # 出力ディレクトリ
│   ├── frames/                         # 抽出フレーム（一時）
│   └── analysis/                       # 解析結果CSV
├── templates/                          # 分類用テンプレート
├── config.json                         # アプリ設定（自動生成）
├── README.md                           # 本ドキュメント
└── TAURI_MIGRATION_DESIGN.md           # Tauri移植設計書
```

## 入力インジケータの仕様

### 解析対象領域
- **解析領域**: GUIアプリの「解析領域設定」メニューで指定
  - 左上座標 (X, Y)
  - タイルサイズ（正方形、例: 48x48ピクセル）
  - 列数（例: 6または7列）
- **解析対象**: 最下行（最新入力行）のみ
- **列構成例**: [継続フレーム数] [方向キー] [A1] [A2] [B] [W] [Start]

### 重要な仕様
- **継続フレーム数**: 画像からは読み取らず、連続する同一入力を独自にカウント
- **方向入力**: テンキー配列（1=左下、2=下、...、9=右上、5=ニュートラル）
- **列構成**: 列0は継続フレーム数表示（解析では無視）、列1以降を解析対象とする
- **解析対象列**: 方向1個＋ボタン（ユーザー指定の数）

### CSV出力フォーマット

```csv
継続フレーム数,方向入力,A1,A2,B,W,Start
1,5,0,0,0,0,0
3,6,1,0,0,0,0
2,5,1,1,0,0,0
```

- 継続フレーム数: 1〜255（連続する同一入力のフレーム数）
- 方向入力: 1〜9（テンキー配列、5=ニュートラル）
- ボタン: 0=押されていない、1=押している

## 機械学習モデルの詳細

### アーキテクチャ

```
Input (48x48 RGB)
  ↓
Conv2D(3→32, 3x3) + ReLU + MaxPool(2x2)  # 24x24
  ↓
Conv2D(32→64, 3x3) + ReLU + MaxPool(2x2) # 12x12
  ↓
Conv2D(64→128, 3x3) + ReLU + MaxPool(2x2) # 6x6
  ↓
Flatten (4608)
  ↓
Linear(4608→512) + ReLU + Dropout(0.5)
  ↓
Linear(512→14)  # 14クラス分類
  ↓
Output (Softmax)
```

### 学習設定

- **オプティマイザ**: Adam (lr=0.001)
- **損失関数**: CrossEntropyLoss
- **バッチサイズ**: 32
- **エポック数**: 20（デフォルト）
- **検証分割**: 20%
- **データ拡張**: なし（現状）

### クラス順序（重要）

モデルの出力順序は以下の通り：
```
0-7: dir_1, dir_2, dir_3, dir_4, dir_6, dir_7, dir_8, dir_9（固定）
8〜: <ボタンクラス>（モデル学習メニューで指定した順序）
最後: others（固定）
```

**例**: モデル学習メニューで btn_a1 → btn_a2 → btn_b → btn_w → btn_start の順に指定した場合
```
0: dir_1, 1: dir_2, 2: dir_3, 3: dir_4, 4: dir_6,
5: dir_7, 6: dir_8, 7: dir_9
8: btn_a1, 9: btn_a2, 10: btn_b, 11: btn_w, 12: btn_start
13: others
```

**重要**:
- `dir_1`〜`dir_9`（dir_5除く）と`others`の位置は固定
- ボタンクラスの順序はGUIアプリの「モデル学習」メニューで指定
- 指定した順序は`training_data/buttons.txt`に保存され、次回以降も使用されます

## パフォーマンス

### 推論速度（参考値）

| デバイス | 1フレーム処理時間 | 備考 |
|---------|----------------|------|
| RTX 3060 | 約20ms | GPU推論（6タイル×CNNフォワード） |
| CPU (Ryzen 5) | 約150ms | CPUフォールバック |

### メモリ使用量

- **GPU VRAM**: 約500MB（モデル+推論バッファ）
- **RAM**: 約200MB（フレームバッファ+GUI）

**ストリーミング処理の効果**:
- 1フレームずつ処理するため、動画の長さに関わらずメモリ使用量が一定
- 10分間の動画（36,000フレーム）でもRAM使用量は変わらず
- 従来のバッチ処理（全フレームをメモリ上に保持）と比較し、95%以上のメモリ削減

## トラブルシューティング

### GStreamerエラー
```
Error: filesrcの作成に失敗しました
```
→ GStreamerが正しくインストールされ、環境変数`GSTREAMER_1_0_ROOT_MSVC_X86_64`が設定されているか確認

### モデル読み込みエラー
```
Error: モデルの読み込みに失敗
```
→ tar.gz形式のモデルファイルを使用してください。ファイルが破損していないか、正しい形式で保存されているか確認してください

### 動画解像度エラー
```
Error: 動画解像度がモデルのメタデータと一致しません
```
→ モデル学習時と同じ解像度（1920x1080推奨）の動画を使用してください

### GPU認識されない
```
Warning: Using CPU backend
```
→ WGPUが正しくインストールされているか、GPUドライバが最新か確認してください

### 方向入力がすべてN（5）になる
```
すべての方向がニュートラル
```
→ 学習データのクラス順序が正しいか確認。`training_data/buttons.txt`の順序とモデル学習時の順序が一致している必要があります

## 今後の予定

- [x] フレーム抽出機能
- [x] 機械学習モデル（Burn + WGPU）
- [x] 入力履歴抽出（1フレームずつ順次処理）
- [x] GUIアプリケーション
- [x] CSV編集機能
- [x] モデルメタデータ管理
- [x] GPU/CPU両対応
- [ ] データ拡張（学習時）
- [ ] バッチ推論最適化
- [ ] Tauri移植（クロスプラットフォーム対応）
- [ ] リアルタイム動画解析
- [ ] 統計情報表示（APM、入力頻度など）

## ライセンス

MIT License

## 貢献

プルリクエストを歓迎します！バグ報告や機能要望はIssuesでお願いします。
